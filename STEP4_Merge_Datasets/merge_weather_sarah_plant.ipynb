{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e198206",
   "metadata": {},
   "source": [
    "# Data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "aa4fc16c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Sarah paths\n",
    "#path_sat=\"/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SARAH_DATA/DNI/ORD49156\"\n",
    "path_sat=\"/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SARAH_DATA/SIS/ORD49425\"\n",
    "# Era5 \n",
    "path_era=\"/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/ERA5_DATA/\"\n",
    "# On site data:   \n",
    "path_site='/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SCRIPTS/ONSITE_DATA'\n",
    "prefix = \"DATA_SK\"  # The prefix you want to match\n",
    "\n",
    "\n",
    "## Solar Kapitla names \n",
    "# names=( agioi10 Mavroneri Sitia  Assiros Palaiokastro  Larisa Mesa1 Arnissa Radi Lagos )\n",
    "# names working: agioi10, Sitia, Palaiokastro, Lagos \n",
    "locs= [ \"Sitia\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "25f2adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sctipt merges Sarah csv data and data from solar kapital platforms \n",
    "import os \n",
    "import pandas as pd \n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba76abb3",
   "metadata": {},
   "source": [
    "# Read the ERA5 Land data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7223b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert all files from tsv to csv and save them locally with _COMMA.csv \n",
    "#locs= [ \"Lagos\" , \"mavroneri\" ]\n",
    "names = [ \"Tmax_2T_\", \"Tmean_2T_\", \"Tmin_2T_\" ]\n",
    "for loc in locs: \n",
    "    for name in names: \n",
    "        fname1= name+loc+\".csv\"\n",
    "        fname1b= name+loc+\"_COMMA.csv\"\n",
    "        #print(fname1)\n",
    "        path1=os.path.join(path_era,fname1 )\n",
    "        path1_csv = os.path.join(path_era,fname1b )\n",
    "        #print(path1)\n",
    "        #print(path1_csv)\n",
    "        # Convert to comma delimited csv \n",
    "        with open(path1, 'r') as tsvfile, open(path1_csv, 'w', newline='') as csvfile:\n",
    "                #print(tsvfile, name, loc)\n",
    "                #print(csvfile, name, loc)\n",
    "                tsvreader = csv.reader(tsvfile, delimiter='\\t')\n",
    "                csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "                for row in tsvreader:\n",
    "                    new_row = []\n",
    "                    for item in row:\n",
    "                      # Split each item based on one or more spaces\n",
    "                      items = re.split('\\s+', item)\n",
    "                      # Join the items using a comma separator\n",
    "                      new_row.append(','.join(items))\n",
    "                    csvwriter.writerow(new_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d35abad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame for:  Tmax_2T_Sitia\n",
      "Data frame for:  Tmean_2T_Sitia\n",
      "Data frame for:  Tmin_2T_Sitia\n"
     ]
    }
   ],
   "source": [
    "# Read in the csv files and create one \n",
    "# data frame per location and Tmean, Tmax, Tmin \n",
    "#locs= [ \"Lagos\" , \"mavroneri\" ]\n",
    "names = [ \"Tmax_2T_\", \"Tmean_2T_\", \"Tmin_2T_\" ]\n",
    "dfs={}\n",
    "for item in locs: \n",
    "    count=0\n",
    "    for name in names: \n",
    "        fname1= name+item+\".csv\"\n",
    "        fname1b= name+item+\"_COMMA.csv\"\n",
    "                #print(fname1)\n",
    "        path1=os.path.join(path_era,fname1 )\n",
    "        path1_csv = os.path.join(path_era,fname1b )\n",
    "        column_names=[ 'empty1', 'empty2', 'empty3', 'empty4', 'empty5', 'name' , 'date', 'time', 'lon', 'lat', 'value', 'empty12'\\\n",
    "             , 'empty13', 'empty14', 'empty15', 'empty16', 'empty17']\n",
    "        # Read the svaed comma delimeted csv with assigned column names\n",
    "        #df2 = pd.read_csv(path1_csv, sep=',', names=column_names)\n",
    "        c_name=name+item\n",
    "        print(\"Data frame for: \", c_name)\n",
    "        dfs[c_name]=pd.read_csv(path1_csv, sep=',', names=column_names)\n",
    "        \n",
    "        #Test commands to check if there are nan in columns (empty3 in this example) \n",
    "        #and count the number of nan per column.\n",
    "        dfs[c_name]['empty3'].isnull().values.any()\n",
    "        dfs[c_name]['empty3'].isnull().sum()\n",
    "       # Drop the 'county_name' and 'state' columns\n",
    "        dfs[c_name]=dfs[c_name].drop(['empty1', 'time', 'lon', 'lat', 'value', 'empty12', 'empty13', 'empty14', \\\n",
    "             'empty15', 'empty16', 'empty17'], axis='columns')\n",
    "       # Drop the two rows that contains cdo output message from previous step \n",
    "        dfs[c_name] = dfs[c_name].tail(-2)\n",
    "        # Rename columns\n",
    "        dfs[c_name].rename(columns = {'empty2':'Name', 'empty3':'Date', 'empty4': 'time', \\\n",
    "              'empty5':'lon', 'name':'lat', 'date': c_name}, inplace = True)\n",
    "        #print(dfs[c_name])\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b07efb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the dataframes and concatenate for  the same location the Tmax, Tmin, Tmean \n",
    "\n",
    "#dfs['Tmax_2T_agioi10']\n",
    "#locs =  [ \"Lagos\" , \"mavroneri\" ]\n",
    "dfs_n={}\n",
    "for name in locs: \n",
    "    #dfs_n[name] = pd.concat([dfs['Tmax_2T_agioi10'], dfs['Tmean_2T_agioi10']  ])\n",
    "    new_name=\"Tmax_2T_\"+name\n",
    "    new_name2=\"Tmean_2T_\"+name\n",
    "    new_name3=\"Tmin_2T_\"+name\n",
    "    dfs[new_name].reset_index(drop=True, inplace=True)\n",
    "    dfs[new_name2].reset_index(drop=True, inplace=True)\n",
    "    dfs[new_name3].reset_index(drop=True, inplace=True)\n",
    "    dfs_n[name] = pd.concat([dfs[new_name], dfs[new_name2], dfs[new_name3] ], axis=1) \n",
    "    # Drop duplicate columns \n",
    "    dfs_n[name] = dfs_n[name].T.drop_duplicates().T\n",
    "#print(dfs_n)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a5e6a1",
   "metadata": {},
   "source": [
    "#  Read in sarah satellite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "84c727af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to comma delimited csv \n",
    "## Convert all files from tsv to csv and save them locally with _COMMA.csv \n",
    "#merged_GHI_0_COMMA.csv\n",
    "#locs= [ \"Lagos\" , \"mavroneri\" ]\n",
    "count=0\n",
    "for loc in locs: \n",
    "    for name in names: \n",
    "        count=count +1 \n",
    "        count2=str(count)\n",
    "        fname2= \"merged_GHI_\"+count2+\".csv\"\n",
    "        fname2b= \"merged_GHI_\"+loc+\"_COMMA.csv\"\n",
    "        #print(fname1)\n",
    "        path2=os.path.join(path_sat,fname2 )\n",
    "        path2_csv = os.path.join(path_sat,fname2b )\n",
    "        #print(path1)\n",
    "        #print(path1_csv)\n",
    "        # Convert to comma delimited csv \n",
    "        with open(path2, 'r') as tsvfile, open(path2_csv, 'w', newline='') as csvfile:\n",
    "                #print(tsvfile, name, loc)\n",
    "                #print(csvfile, name, loc)\n",
    "                tsvreader = csv.reader(tsvfile, delimiter='\\t')\n",
    "                csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "                for row in tsvreader:\n",
    "                    new_row = []\n",
    "                    for item in row:\n",
    "                      # Split each item based on one or more spaces\n",
    "                      items = re.split('\\s+', item)\n",
    "                      # Join the items using a comma separator\n",
    "                      new_row.append(','.join(items))\n",
    "                    csvwriter.writerow(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "c1cea7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in the csv nad pass it to dataframe per location\n",
    "\n",
    "#locs= [ \"Lagos\" , \"mavroneri\" ]\n",
    "dfs2={}\n",
    "for item in locs: \n",
    "    count=0 \n",
    "    fname2b= \"merged_GHI_\"+item+\"_COMMA.csv\"     \n",
    "    path2_csv = os.path.join(path_sat,fname2b )\n",
    "    import numpy as np\n",
    "    # name the columns so that it can split each row accordingly \n",
    "    column_names=[ 'empty1', 'empty2', 'empty3', 'empty4', 'empty5', 'name' , 'date', 'time', 'lon', 'lat', 'value', 'empty12'\\\n",
    "             , 'empty13', 'empty14', 'empty15', 'empty16', 'empty17']\n",
    "    # Read the svaed comma delimeted csv with assigned column names\n",
    "    name=item \n",
    "    dfs2[name] = pd.read_csv(path2_csv, sep=',', names=column_names)\n",
    "    # Test commands to check if there are nan in columns (empty3 in this example) \n",
    "    # and count the number of nan per column.\n",
    "    dfs2[name]['empty3'].isnull().values.any()\n",
    "    dfs2[name]['empty3'].isnull().sum()\n",
    "    # Drop the 'county_name' and 'state' columns\n",
    "    dfs2[name]=dfs2[name].drop(['empty1', 'empty2', 'time', 'lon', 'lat', 'value', 'empty12', 'empty13', 'empty14', \\\n",
    "             'empty15', 'empty16', 'empty17'], axis='columns')\n",
    "    # Drop the two rows that contains cdo output message from previous step \n",
    "    dfs2[name] = dfs2[name].tail(-2)\n",
    "    # Rename columns \n",
    "    name_solar=\"GHI\"+item\n",
    "    dfs2[name].rename(columns = {'empty3':'Date', 'empty4':'Time', 'empty5': 'lon', \\\n",
    "                      'name':'lat', 'date':name_solar}, inplace = True)\n",
    "\n",
    "#dfs2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21842294",
   "metadata": {},
   "source": [
    "# Concatenate Satellite and Era5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "96182bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saellite dfs are named: dfs2 and ERA5: dfs_n\n",
    "#locs= [ \"Lagos\" , \"mavroneri\" ]\n",
    "df_SE={}\n",
    "for name in locs: \n",
    "    dfs2[name].reset_index(drop=True, inplace=True)\n",
    "    dfs_n[name].reset_index(drop=True, inplace=True)\n",
    "    # Ensure the date columns are in datetime format]\n",
    "    # Remove 'nearest' string Assuming df3 is your DataFrame\n",
    "    dfs2[name] = dfs2[name][dfs2[name]['Date'] != 'Nearest']\n",
    "    dfs_n[name] = dfs_n[name][dfs_n[name]['Date'] != 'Nearest']\n",
    "    # Ensure the date column is in datetime format\n",
    "    dfs_n[name]['Date'] = pd.to_datetime(dfs_n[name]['Date'])\n",
    "    dfs2[name]['Date'] = pd.to_datetime(dfs2[name]['Date'])\n",
    "    # Merge the two DataFrames only where the dates coincide\n",
    "    df_SE[name] = pd.merge( dfs_n[name], dfs2[name], on='Date')\n",
    "  \n",
    "    # Drop uncessary columns \n",
    "    df_SE[name]=df_SE[name].drop(['Time', 'time', 'lon_y', 'lat_y', 'Name'], axis='columns')\n",
    "    #print(df_SE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b481b1ed",
   "metadata": {},
   "source": [
    "#  Concatenate df_SE with on site data  : df_SEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "75bf12da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On site data:   \n",
    "# path_site=\"/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SolarKapital/\"\n",
    "# path_out=\"/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SolarKapital/DATA_SK_1\"\n",
    "\n",
    "\n",
    "# # First merge the infividual site data \n",
    "# files = os.listdir(path_site)\n",
    "# files_csv = [f for f in files if f[-3:] == 'csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ec45e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "def find_directories_with_prefix(path, prefix):\n",
    "    dirs = []\n",
    "    for entry in os.listdir(path):\n",
    "        full_entry_path = os.path.join(path, entry)\n",
    "        #print(full_entry_path)\n",
    "        if os.path.isdir(full_entry_path) and entry.startswith(prefix):\n",
    "            dirs.append(full_entry_path)\n",
    "            #print(full_entry_path)\n",
    "    return dirs\n",
    "\n",
    "# Usage\n",
    "# The path to the parent directory you want to search in\n",
    "#path_site='/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/SCRIPTS/ONSITE_DATA'\n",
    "#prefix = \"DATA_SK\"  # The prefix you want to match\n",
    "\n",
    "matching_directories = find_directories_with_prefix(path_site, prefix)\n",
    "df_SK={}\n",
    "df_SK2={}\n",
    "df_names=[]\n",
    "max_cols=[]\n",
    "for directory in matching_directories:\n",
    "    #print(f'Directory found: {directory}')\n",
    "    names=os.path.basename(directory)\n",
    "    #print(names)\n",
    "    last_part = names.split('_', maxsplit=2)[2]\n",
    "    df_names.append(last_part)\n",
    "    files = os.listdir(directory)\n",
    "    files_csv = [f for f in files if f[-3:] == 'csv']\n",
    "    prefix=last_part\n",
    "    path_out=os.path.join(path_site, prefix)\n",
    "    df_SK[last_part] = pd.DataFrame()\n",
    "    #print(last_part)\n",
    "    for f in files_csv:\n",
    "        f2=os.path.join(directory,f)\n",
    "        #print(f2)\n",
    "        data=pd.read_table(f2,  header=None, skiprows=1)\n",
    "        df_SK[last_part] = pd.concat([df_SK[last_part], pd.DataFrame.from_records(data)])\n",
    "    df_SK[last_part] = df_SK[last_part].reset_index(drop=True)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6ebae816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sitia':            Date Yield(Kwh)\n",
       " 916  2017-12-31    106.179\n",
       " 915  2017-12-30    221.722\n",
       " 914  2017-12-29    125.629\n",
       " 913  2017-12-28    229.113\n",
       " 912  2017-12-27     64.480\n",
       " ...         ...        ...\n",
       " 610  2011-01-03    257.300\n",
       " 824  2011-01-03    257.300\n",
       " 0    2011-01-03    257.300\n",
       " 1802 2011-01-01           \n",
       " 1465 2011-01-01           \n",
       " \n",
       " [2354 rows x 2 columns]}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_SK2={}\n",
    "#names =  [ \"Lagos\" ]\n",
    "for name in locs: \n",
    "    #print(df_SK[name], name)\n",
    "    max_cols = max(df_SK[name].apply(lambda x: x.str.count(';').sum() + 1, axis=1))\n",
    "    #print(max_cols)\n",
    "    df_SK2[name] = pd.DataFrame(columns=range(max_cols))\n",
    "    # Split each row into the corresponding number of columns\n",
    "    for i, row in df_SK[name].iterrows():\n",
    "            #print('Working on row', i)\n",
    "            values = row.str.split(';', expand=True)\n",
    "            #print(values)\n",
    "            df_SK2[name].loc[i, :len(values.columns)-1] = values.values[0]\n",
    "    # Rename columns \n",
    "    new_column_names = { 0: 'Date', 1: 'Yield(Kwh)'}   \n",
    "    df_SK2[name] = df_SK2[name].rename(columns=new_column_names)        \n",
    "    # Ensure the date column is in datetime format\n",
    "    df_SK2[name]['Date'] = pd.to_datetime(df_SK2[name]['Date'])\n",
    "    df_SK2[name] = df_SK2[name].sort_values(by='Date', ascending=False)\n",
    "df_SK2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "775c0e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "{'Sitia':            Date   lon_x    lat_x Tmax_2T_Sitia Tmean_2T_Sitia Tmin_2T_Sitia  \\\n",
      "0    2011-01-01  26.125  35.1316      11.75317       10.84762      10.16821   \n",
      "1    2011-01-01  26.125  35.1316      11.75317       10.84762      10.16821   \n",
      "2    2011-01-03  26.125  35.1316      12.91787       12.30602      11.26736   \n",
      "3    2011-01-03  26.125  35.1316      12.91787       12.30602      11.26736   \n",
      "4    2011-01-03  26.125  35.1316      12.91787       12.30602      11.26736   \n",
      "...         ...     ...      ...           ...            ...           ...   \n",
      "1647 2015-12-27  26.125  35.1316      15.06137       13.97433      13.05648   \n",
      "1648 2015-12-28  26.125  35.1316      14.70553       13.86932       13.1332   \n",
      "1649 2015-12-29  26.125  35.1316      14.12356       13.32867      12.67709   \n",
      "1650 2015-12-30  26.125  35.1316      13.25308       11.70761      9.042475   \n",
      "1651 2015-12-31  26.125  35.1316      8.441431       5.357202      3.832483   \n",
      "\n",
      "     GHISitia Yield(Kwh)  \n",
      "0         127             \n",
      "1         127             \n",
      "2          98    257.300  \n",
      "3          98    257.300  \n",
      "4          98    257.300  \n",
      "...       ...        ...  \n",
      "1647      118    341.524  \n",
      "1648      126    172.556  \n",
      "1649      131    239.444  \n",
      "1650      112    144.121  \n",
      "1651       46     48.420  \n",
      "\n",
      "[1652 rows x 8 columns]}\n"
     ]
    }
   ],
   "source": [
    "# Concartenate all\n",
    "# Saellite dfs are named: dfs2 and ERA5: dfs_n\n",
    "df_SEL={}\n",
    "merged_df={}\n",
    "data_frames={}\n",
    "for name in locs:\n",
    "    if name in df_SE and name in df_SK2:\n",
    "        print(\"OK\")\n",
    "        df_SK2[name]['Date'] = pd.to_datetime(df_SK2[name]['Date'])\n",
    "        # Merge the two DataFrames only where the dates coincide\n",
    "        df_SEL[name] = pd.merge( df_SE[name], df_SK2[name], on='Date')     \n",
    "        #df_SE[name]=df_SE[name].drop(['Time', 'time', 'lon_y', 'lat_y', 'Name'], axis='columns')\n",
    "        \n",
    "print(df_SEL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647da158",
   "metadata": {},
   "source": [
    "# Create labels for data and save the data into a csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c12bf755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the upper and lower bounds  of yield data that are out in the 9 and 95 percintile.\n",
    "# import the packages\n",
    "import numpy as np\n",
    "# chack the outliers for yield \n",
    "data=df_SEL['Sitia']['Yield(Kwh)']\n",
    "data2=data.to_numpy()\n",
    "# filter out empty strings \n",
    "data_rm = list(filter(None, data2))\n",
    "arr = np.array(data_rm)\n",
    "# Convert from string to float \n",
    "data3=arr.astype(float)\n",
    "\n",
    "#print(np.where(data3))\n",
    "\n",
    "q5, q95 = np.percentile(data3, [5, 95])\n",
    "#print(q1, q3)\n",
    "# iqr = q3 - q1\n",
    "# lower_bound = q1 - 1.5*iqr\n",
    "# upper_bound = q3 + 1.5*iqr\n",
    "#print(data3)\n",
    "lower_bound= q5\n",
    "upper_bound=q95\n",
    "\n",
    "# for index, element in enumerate(data3):\n",
    "#     print(f\"Index: {index}, Element: {element}\")\n",
    "\n",
    "outliers = np.where((data3 < lower_bound) | (data3 > upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5d0ce342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for the data, 1 if the exceed the upper and lower bounds and 0 if they are within limits.\n",
    "# Define a custom function that checks a condition and returns a value accordingly\n",
    "def check_condition(row):\n",
    "    #row['Yield(Kwh)'] = row['Yield(Kwh)'].astype(float)\n",
    "    row['Yield(Kwh)'] = pd.to_numeric(row['Yield(Kwh)'])\n",
    "    if row['Yield(Kwh)'] <= lower_bound:\n",
    "        return 1\n",
    "    elif row['Yield(Kwh)'] >= upper_bound:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "# Use the apply() function with the custom function to create a new column 'C'\n",
    "df_SEL['Sitia']['label'] = df_SEL['Sitia'].apply(check_condition, axis=1)\n",
    " \n",
    "#df_SEL['agioi10'][0:50]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cc0ec7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/DATA_FOR_TRAINING\n",
    "df_SEL['Sitia'].to_csv('/Users/paraskevivourlioti/Documents/NEURALIO/KYKLOS/DATA_KYKLOS/DATA_FOR_TRAINING/SK_Sitia.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e273359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 tenspr new",
   "language": "python",
   "name": "tensorflow_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
